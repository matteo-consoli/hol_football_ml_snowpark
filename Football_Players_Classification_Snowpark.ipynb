{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8c325d-2c7d-4a9c-999d-e23f66ef6df7",
   "metadata": {},
   "source": [
    "# Football Player Classification\n",
    "## Hands On Lab : Snowflake & Snowpark for End to end flow\n",
    "\n",
    "This is an all-in-one Jupyter notebook covering the entire pipeline for football player classification in Snowpark. \n",
    "\n",
    "The only step not included in this notebook is downloading the dataset from Kaggle which I recommend doing it directly from the source, it's free and it will take just a few seconds! Download the football dataset [here](https://www.kaggle.com/datasets/davidcariboo/player-scores).\n",
    "\n",
    "\n",
    "In this notebook, we will start by creating database objects and feature engineering views on Snowflake. \n",
    "\n",
    "Once the data is prepared, we'll use Snowpark to train and test a random forest model to predict the football player's position. \n",
    "\n",
    "NOTE: Model accuracy is not the primary goal of this notebook; instead, it aims to provide a comprehensive, step-by-step example (on a very cool use case).\n",
    "\n",
    "![Introduction - Design](https://github.com/matteo-consoli/hol_football_ml_snowpark/blob/main/images/design_football_ml_dark.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c283080-23dd-4109-961c-651da6942ab5",
   "metadata": {},
   "source": [
    "# Data Ingestion & Feature Engineering\n",
    "\n",
    "**Pre-Reqs**: You completed the Pre-Reqs setup. You have your conda env configured. Jupyter Notebook and libraries are installed. Let's start!\n",
    "\n",
    "#### Step 1: Load Account Credentials\n",
    "\n",
    "Modify the `config.json` file with your Snowflake account credentials . The file should look like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user\": \"your_username\",\n",
    "  \"password\": \"your_password\",\n",
    "  \"account\": \"org-accountname\",\n",
    "  \"role\": \"your_role\",\n",
    "}\n",
    "```\n",
    "\n",
    "Otherwise, adjust the code below to pass connection parameters directly in the notebook. It's not considered a good practice, but I won't judge you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68150df9-216f-4614-aea9-249ea42cb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import json\n",
    "import sys\n",
    "import cachetools\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Import Snowflake modules\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e55863-eb63-4f11-955f-75abf1f80793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get account credentials from a json file\n",
    "with open(\"config.json\") as f:\n",
    "    data = json.load(f)\n",
    "    username = data[\"user\"]\n",
    "    password = data[\"password\"]\n",
    "    account = data[\"account\"]\n",
    "    role = data[\"role\"]\n",
    "# If you don't want to use the config.json, I can feel you. \n",
    "# In that case, comment the line above and write the connection_parameters directly below as strings. \n",
    "\n",
    "# Specify connection parameters\n",
    "connection_parameters = {\n",
    "    \"account\": account,\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"role\": role\n",
    "}\n",
    "\n",
    "# Create Snowpark session\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b57dd4-b71d-4f19-a73c-b722f63216d8",
   "metadata": {},
   "source": [
    "#### Step 2: Create Objects in Snowflake\n",
    "\n",
    "Once connected, we execute SQL commands directly from this notebook to create the database, schema, tables, and stages. Although not the most convenient option, it allows you to follow the entire pipeline directly from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8061e80-e37b-4b74-8d2d-36957593e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"CREATE DATABASE IF NOT EXISTS HOL_DB;\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS HOL_DB.HOL_SNOWPARK_FOOTBALL;\").collect()\n",
    "session.sql(\"CREATE STAGE IF NOT EXISTS HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_DATASET_STAGE;\").collect()\n",
    "session.sql(\"CREATE STAGE IF NOT EXISTS HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO\").collect()\n",
    "session.sql(\"USE SCHEMA HOL_DB.HOL_SNOWPARK_FOOTBALL;\").collect()\n",
    "session.sql(\"\"\"\n",
    "    CREATE WAREHOUSE IF NOT EXISTS HOL_WH\n",
    "    WAREHOUSE_SIZE = 'XSmall' \n",
    "    AUTO_SUSPEND=60 \n",
    "    AUTO_RESUME=True;\n",
    "\"\"\").collect()\n",
    "session.sql(\"USE WAREHOUSE HOL_WH;\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT HOL_DB.HOL_SNOWPARK_FOOTBALL.HOL_FOOTBALL_CSV_FILE_FORMAT\n",
    "    TYPE = CSV\n",
    "    FIELD_DELIMITER = ','\n",
    "    FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "    PARSE_HEADER = True\n",
    "    TRIM_SPACE = TRUE\n",
    "    NULL_IF = ('NULL', 'null')\n",
    "    ESCAPE_UNENCLOSED_FIELD= NONE\n",
    "    ERROR_ON_COLUMN_COUNT_MISMATCH=false;\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfab57-6a82-4c2d-b3c6-7995343affc4",
   "metadata": {},
   "source": [
    "#### Step 3: Download the Dataset and Load Files into Snowflake Stage\n",
    "\n",
    "- Download the dataset from [Kaggle](https://www.kaggle.com/datasets/davidcariboo/player-scores).\n",
    "- Define the local directory where you saved the downloaded dataset. Adjust the `localpath` variable accordingly:\n",
    "\n",
    "```python\n",
    "localpath = '/Users/mconsoli/Downloads/archive/dataset'\n",
    "```\n",
    "This step will push all the files into the Snowflake stage, it might take a while. \n",
    "Standard Ingestion approaches for Snowflake usually are usually: Batch Loading (e.g. from S3 buckets for AWS), Snowpipe, Snowpipe Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0ef7f-044d-45ac-8867-1171a396526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "localpath= '/Users/mconsoli/Downloads/archive/dataset'\n",
    "\n",
    "files = [\n",
    "    \"club_games.csv\",\n",
    "    \"appearances.csv\",\n",
    "    \"clubs.csv\",\n",
    "    \"competitions.csv\",\n",
    "    \"game_events.csv\",\n",
    "    \"game_lineups.csv\",\n",
    "    \"games.csv\",\n",
    "    \"players.csv\",\n",
    "    \"player_valuations.csv\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    filepath = f\"{localpath}/{file}\"\n",
    "    stage_path = f\"@HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_DATASET_STAGE/\"\n",
    "    put_command = f\"PUT 'file://{filepath}' '{stage_path}' AUTO_COMPRESS=false OVERWRITE=true\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading '{file}' to stage.\")\n",
    "        result = session.sql(put_command).collect()\n",
    "        print(f\"File '{file}' successfully uploaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file '{file}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f50ad-fef8-48ad-aafd-091f2ee9db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(session.sql(\"LIST @HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_DATASET_STAGE\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8273a-8f73-4a5b-8360-22f0899b71f4",
   "metadata": {},
   "source": [
    "#### Step 4: Load Files into Snowflake Tables\n",
    "\n",
    "Now that the dataset files are in the Snowflake stage, the next step is to create tables and load the data. Execute the following SQL script in Snowflake to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c980f7e-fa10-4fa0-85cb-fd6f0b63db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"club_games\", \"appearances\", \"clubs\", \"competitions\", \"game_events\", \"game_lineups\", \"games\", \"players\", \"player_valuations\"]\n",
    "for table in tables:\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE HOL_DB.HOL_SNOWPARK_FOOTBALL.{table.upper()}\n",
    "        USING TEMPLATE (\n",
    "            SELECT ARRAY_AGG(object_construct(*))\n",
    "            FROM TABLE(\n",
    "                INFER_SCHEMA(\n",
    "                    LOCATION=>'@FOOTBALL_DATASET_STAGE/{table}.csv',\n",
    "                    FILE_FORMAT=>'HOL_DB.HOL_SNOWPARK_FOOTBALL.HOL_FOOTBALL_CSV_FILE_FORMAT',\n",
    "                    IGNORE_CASE => TRUE\n",
    "                )\n",
    "            )\n",
    "        );\n",
    "    \"\"\").collect()\n",
    "    session.sql(f\"\"\"\n",
    "        COPY INTO HOL_DB.HOL_SNOWPARK_FOOTBALL.{table.upper()}\n",
    "        FROM '@FOOTBALL_DATASET_STAGE/{table}.csv'\n",
    "        FILE_FORMAT = HOL_DB.HOL_SNOWPARK_FOOTBALL.HOL_FOOTBALL_CSV_FILE_FORMAT\n",
    "        MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;\n",
    "    \"\"\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9fa0d1-1173-4b30-b6da-dd03b056fe49",
   "metadata": {},
   "source": [
    "#### Step 5: Feature Engineering\n",
    "\n",
    "Given the small the dataset, a straightforward approach for feature engineering is creating views that consolidate player statistics and encode the POSITION (target column) as integers.\n",
    "\n",
    "**Approach**\n",
    "\n",
    "1. **Consolidating Player Statistics:**\n",
    "   - Views like `PLAYER_VALUATION_STATS_VIEW` aggregate player statistics, including total goals, appearances, yellow cards, red cards, assists, and the latest market value.\n",
    "   - A common theme across these views is to extract meaningful insights from the available player data.\n",
    "\n",
    "2. **Position Encoding:**\n",
    "   - The `PLAYER_AGGREGATE_STATS_DETAILS_VIEW` involves encoding the POSITION column as an integer for model compatibility.\n",
    "   - Positions like 'Goalkeeper,' 'Defender,' 'Midfield,' and 'Attack' are numerically encoded to facilitate machine learning tasks.\n",
    "\n",
    "**Encoding Legenda**\n",
    "- WHEN P.POSITION = 'Goalkeeper' THEN 0\n",
    "- WHEN P.POSITION = 'Defender' THEN 1\n",
    "- WHEN P.POSITION = 'Midfield' THEN 2\n",
    "- WHEN P.POSITION = 'Attack' THEN 3\n",
    "\n",
    "![Feature Engineering - Views Diagram](https://github.com/matteo-consoli/hol_football_ml_snowpark/blob/main/images/design_footbal_ml_feature_enginnering_dark.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19146255-9674-471f-9355-cb21f504f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View: PLAYER_VALUATION_STATS_VIEW\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATION_STATS_VIEW AS\n",
    "WITH LatestMarketValue AS (\n",
    "    SELECT\n",
    "        PLAYER_ID,\n",
    "        EXTRACT(YEAR FROM DATE) AS YEAR,\n",
    "        MAX(DATETIME) AS LATEST_DATETIME\n",
    "    FROM\n",
    "        HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATIONS\n",
    "    GROUP BY\n",
    "        PLAYER_ID,\n",
    "        EXTRACT(YEAR FROM DATE)\n",
    ")\n",
    "SELECT\n",
    "    A.PLAYER_ID,\n",
    "    EXTRACT(YEAR FROM A.DATE) AS YEAR,\n",
    "    SUM(A.GOALS) AS TOTAL_GOALS,\n",
    "    COUNT(A.APPEARANCE_ID) AS TOTAL_APPEARANCES,\n",
    "    SUM(A.YELLOW_CARDS) AS TOTAL_YELLOW_CARDS,\n",
    "    SUM(A.RED_CARDS) AS TOTAL_RED_CARDS,\n",
    "    SUM(A.ASSISTS) AS TOTAL_ASSISTS,\n",
    "    COALESCE(P.MARKET_VALUE_IN_EUR, 0) AS LATEST_MARKET_VALUE_IN_EUR\n",
    "FROM\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.APPEARANCES A\n",
    "LEFT JOIN\n",
    "    LatestMarketValue LMV\n",
    "ON\n",
    "    A.PLAYER_ID = LMV.PLAYER_ID\n",
    "    AND EXTRACT(YEAR FROM A.DATE) = LMV.YEAR\n",
    "LEFT JOIN\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATIONS P\n",
    "ON\n",
    "    A.PLAYER_ID = P.PLAYER_ID\n",
    "    AND LMV.LATEST_DATETIME = P.DATETIME\n",
    "GROUP BY\n",
    "    A.PLAYER_ID,\n",
    "    EXTRACT(YEAR FROM A.DATE),\n",
    "    P.MARKET_VALUE_IN_EUR;\n",
    "\"\"\").collect()\n",
    "\n",
    "# View: PLAYER_COMPETITION_YEAR_STATS_DETAILS_VIEW\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_COMPETITION_YEAR_STATS_DETAILS_VIEW AS\n",
    "SELECT\n",
    "    A.PLAYER_ID,\n",
    "    A.COMPETITION_ID,\n",
    "    C.NAME AS COMPETITION_NAME,\n",
    "    C.SUB_TYPE AS COMPETITION_SUB_TYPE,\n",
    "    C.TYPE AS COMPETITION_TYPE,\n",
    "    EXTRACT(YEAR FROM A.DATE) AS YEAR,\n",
    "    COUNT(A.APPEARANCE_ID) AS APPEARANCES_IN_COMPETITION,\n",
    "    SUM(A.GOALS) AS TOTAL_GOALS,\n",
    "    SUM(A.ASSISTS) AS TOTAL_ASSISTS,\n",
    "    SUM(A.YELLOW_CARDS) AS TOTAL_YELLOW_CARDS,\n",
    "    SUM(A.RED_CARDS) AS TOTAL_RED_CARDS\n",
    "FROM\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.APPEARANCES A\n",
    "JOIN\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.GAMES G\n",
    "ON\n",
    "    A.GAME_ID = G.GAME_ID\n",
    "JOIN\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.COMPETITIONS C\n",
    "ON\n",
    "    A.COMPETITION_ID = C.COMPETITION_ID\n",
    "GROUP BY\n",
    "    A.PLAYER_ID,\n",
    "    A.COMPETITION_ID,\n",
    "    C.NAME,\n",
    "    C.SUB_TYPE,\n",
    "    C.TYPE,\n",
    "    EXTRACT(YEAR FROM A.DATE);\n",
    "\"\"\").collect()\n",
    "\n",
    "# View: PLAYER_VALUATION_DELTA_VIEW\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATION_DELTA_VIEW AS\n",
    "SELECT\n",
    "    PLAYER_ID,\n",
    "    YEAR,\n",
    "    TOTAL_GOALS,\n",
    "    TOTAL_APPEARANCES,\n",
    "    TOTAL_YELLOW_CARDS,\n",
    "    TOTAL_RED_CARDS,\n",
    "    TOTAL_ASSISTS,\n",
    "    LATEST_MARKET_VALUE_IN_EUR,\n",
    "    COALESCE(LAG(LATEST_MARKET_VALUE_IN_EUR) OVER (PARTITION BY PLAYER_ID ORDER BY YEAR), 0) AS PREVIOUS_MARKET_VALUE_IN_EUR,\n",
    "    LATEST_MARKET_VALUE_IN_EUR - COALESCE(LAG(LATEST_MARKET_VALUE_IN_EUR) OVER (PARTITION BY PLAYER_ID ORDER BY YEAR), 0) AS MARKET_VALUE_DELTA,\n",
    "    CASE\n",
    "        WHEN COALESCE(LAG(LATEST_MARKET_VALUE_IN_EUR) OVER (PARTITION BY PLAYER_ID ORDER BY YEAR), 0) != 0\n",
    "        THEN (LATEST_MARKET_VALUE_IN_EUR - COALESCE(LAG(LATEST_MARKET_VALUE_IN_EUR) OVER (PARTITION BY PLAYER_ID ORDER BY YEAR), 0)) / COALESCE(LAG(LATEST_MARKET_VALUE_IN_EUR) OVER (PARTITION BY PLAYER_ID ORDER BY YEAR), 0) * 100\n",
    "        ELSE 0\n",
    "    END AS MARKET_VALUE_DELTA_PERCENT\n",
    "FROM\n",
    "    HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATION_STATS_VIEW;\n",
    "\"\"\").collect()\n",
    "\n",
    "# View: PLAYER_AGGREGATE_STATS_DETAILS_VIEW\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_AGGREGATE_STATS_DETAILS_VIEW AS\n",
    "WITH PlayerDetails AS (\n",
    "    SELECT\n",
    "        P.PLAYER_ID,\n",
    "        P.NAME,\n",
    "        P.CURRENT_CLUB_NAME,\n",
    "        P.CURRENT_CLUB_DOMESTIC_COMPETITION_ID,\n",
    "        P.POSITION,\n",
    "        CASE\n",
    "            WHEN P.POSITION = 'Goalkeeper' THEN 0\n",
    "            WHEN P.POSITION = 'Defender' THEN 1\n",
    "            WHEN P.POSITION = 'Midfield' THEN 2\n",
    "            WHEN P.POSITION = 'Attack' THEN 3\n",
    "            ELSE '' END as POSITION_ENCODED,\n",
    "        P.SUB_POSITION,\n",
    "        P.HEIGHT_IN_CM,\n",
    "        P.LAST_SEASON\n",
    "    FROM\n",
    "        HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYERS P\n",
    "),\n",
    "PlayerAggregatedStats AS (\n",
    "    SELECT\n",
    "        PV.PLAYER_ID,\n",
    "        SUM(PV.TOTAL_GOALS) AS TOTAL_GOALS,\n",
    "        SUM(PV.TOTAL_APPEARANCES) AS TOTAL_APPEARANCES,\n",
    "        SUM(PV.TOTAL_YELLOW_CARDS) AS TOTAL_YELLOW_CARDS,\n",
    "        SUM(PV.TOTAL_RED_CARDS) AS TOTAL_RED_CARDS,\n",
    "        SUM(PV.TOTAL_ASSISTS) AS TOTAL_ASSISTS,\n",
    "        COALESCE(MAX(PV.LATEST_MARKET_VALUE_IN_EUR), 0) AS LATEST_MARKET_VALUE_IN_EUR\n",
    "    FROM\n",
    "        HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_VALUATION_STATS_VIEW PV\n",
    "    GROUP BY\n",
    "        PV.PLAYER_ID\n",
    ")\n",
    "SELECT\n",
    "    PD.PLAYER_ID,\n",
    "    PD.NAME,\n",
    "    PD.CURRENT_CLUB_NAME,\n",
    "    PD.CURRENT_CLUB_DOMESTIC_COMPETITION_ID,\n",
    "    PD.POSITION,\n",
    "    PD.POSITION_ENCODED,\n",
    "    PD.SUB_POSITION,\n",
    "    PD.HEIGHT_IN_CM,\n",
    "    PD.LAST_SEASON,\n",
    "    PA.TOTAL_GOALS,\n",
    "    PA.TOTAL_APPEARANCES,\n",
    "    PA.TOTAL_YELLOW_CARDS,\n",
    "    PA.TOTAL_RED_CARDS,\n",
    "    PA.TOTAL_ASSISTS,\n",
    "    PA.LATEST_MARKET_VALUE_IN_EUR\n",
    "FROM\n",
    "    PlayerDetails PD\n",
    "LEFT JOIN\n",
    "    PlayerAggregatedStats PA\n",
    "ON\n",
    "    PD.PLAYER_ID = PA.PLAYER_ID\n",
    "WHERE PD.POSITION is not null and PD.POSITION != 'Missing';\n",
    "\"\"\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23949-5922-4341-b99d-3c761ed82ac6",
   "metadata": {},
   "source": [
    "# Data Science - Model Training\n",
    "\n",
    "It's time to delve into the ML side! Let's explore how we can train and perform inference in Snowpark / Snowflake. \n",
    "\n",
    "#### Step 1: Create Data Frame with the view containing prepared dataset\n",
    "\n",
    "Define a Snowpark DataFrame based on the view created in the previous step, filtering only players and their statistics for the last season.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ffd7f-43f8-42f2-9cc0-4420fbaa78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpark_df = session.table(\"HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_AGGREGATE_STATS_DETAILS_VIEW\").filter(F.col(\"LAST_SEASON\") == 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e198bbe-8e9b-438e-8a3d-279038ff12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe preview and count records.\n",
    "snowpark_df.show()\n",
    "snowpark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f7a79-d506-4310-8850-90f7a6e10a74",
   "metadata": {},
   "source": [
    "#### Step 2: Split Dataset for Train and Test\n",
    "\n",
    "We split the dataframe to train and test the models on different subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd661cee-419a-4f3b-b46d-efa2ad25a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"USE SCHEMA HOL_DB.HOL_SNOWPARK_FOOTBALL\").collect()\n",
    "\n",
    "train_snowpark_df, test_snowpark_df = snowpark_df.randomSplit([0.8, 0.2])\n",
    "# Save training data\n",
    "train_snowpark_df.write.mode(\"overwrite\").save_as_table(\"HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_AGGREGATE_STATS_DETAILS_TRAIN\")\n",
    "\n",
    "# Save test data\n",
    "test_snowpark_df.write.mode(\"overwrite\").save_as_table(\"HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_AGGREGATE_STATS_DETAILS_TEST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c3233-3cf8-4cfe-868e-213ddbd04f55",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Define Python Function for Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0a51f-eb12-409e-bb28-55f88109d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_classifier(\n",
    "    session: Session,\n",
    "    training_table: str,\n",
    "    feature_cols: list,\n",
    "    target_col: str,\n",
    "    model_name: str,\n",
    "    n_estimators: int = 300,\n",
    "    random_state: int = 100,\n",
    ") -> T.Variant:\n",
    "\n",
    "    # Get training data\n",
    "    df = session.table(training_table).to_pandas()\n",
    "\n",
    "    # Set inputs X and outputs y\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Handle missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Train Random Forest Classifier\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    model.fit(X_imputed, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = pd.DataFrame({\"Feature\": feature_cols, \"Importance\": model.feature_importances_}).to_dict()\n",
    "\n",
    "    # Save model\n",
    "    dump(model, \"/tmp/\" + model_name)\n",
    "    session.file.put(\n",
    "        \"/tmp/\" + model_name,\n",
    "        \"@HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO\",\n",
    "        auto_compress=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # Return a Variant containing feature importances\n",
    "    return {\n",
    "        \"feature_importances\": feature_importances\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe94922-e5d4-4380-915d-0d40e80dd041",
   "metadata": {},
   "source": [
    "#### Step 4: Register Stored Procedure in Snowflake\n",
    "\n",
    "The Stored Procedure is referencing the `train_random_forest_classifier` function defined in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f126d4c-c01b-45d2-9304-6db9600de7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ranfor_snowflake = session.sproc.register(\n",
    "    func=train_random_forest_classifier,\n",
    "    name=\"HOL_DB.HOL_SNOWPARK_FOOTBALL.sproc_train_random_forest\",\n",
    "    is_permanent=True,\n",
    "    replace=True,\n",
    "    stage_location=\"@HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO\",\n",
    "    packages=[\"snowflake-snowpark-python\", \"scikit-learn\", \"joblib\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fab15-786e-4988-85a1-a7e5fe75f5a4",
   "metadata": {},
   "source": [
    "#### Step 5: Train Model in Snowflake\n",
    "\n",
    "Execute SPROC. Model generated is saved in a Snowflake stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9630f-f811-47b5-ba3a-5c09a67d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify inputs\n",
    "training_table = \"HOL_DB.HOL_SNOWPARK_FOOTBALL.PLAYER_AGGREGATE_STATS_DETAILS_TRAIN\"\n",
    "model_name = \"randomforest_player_classification_model.sav\"\n",
    "feature_cols = [\n",
    "    \"TOTAL_GOALS\",\n",
    "    \"TOTAL_APPEARANCES\",\n",
    "    \"TOTAL_YELLOW_CARDS\",\n",
    "    \"TOTAL_RED_CARDS\",\n",
    "    \"TOTAL_ASSISTS\",\n",
    "    \"LATEST_MARKET_VALUE_IN_EUR\"\n",
    "]\n",
    "target_col = \"POSITION_ENCODED\"\n",
    "# Call the training stored procedure\n",
    "feature_contributions = train_ranfor_snowflake(\n",
    "    session, training_table, feature_cols, target_col, model_name,100,42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599af45-075b-47c0-9fca-b3e58938df0f",
   "metadata": {},
   "source": [
    "#### Step 6: Validate Model Feature Importances\n",
    "\n",
    "Check output of the SPROC, visualize Feature Importances and check which is the feature contributing the most to classify player's position.\n",
    "\n",
    "HINT: Surely, NUM_GOALS is important! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ceea4-9472-4ebb-bcd7-4c38f91be509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the string into a dictionary\n",
    "result_dict = json.loads(feature_contributions)\n",
    "\n",
    "# Extract Feature Importances\n",
    "feature_importances = result_dict.get(\"feature_importances\", {})\n",
    "feature_importances_df = pd.DataFrame(feature_importances)\n",
    "print(feature_importances_df)\n",
    "\n",
    "#Optional Command below: list the models stored in Snowflake stage\n",
    "#pd.DataFrame(session.sql(\"LIST @HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd471fc-ac44-4842-893f-3555fd7eaeb4",
   "metadata": {},
   "source": [
    "# Data Science - Model Inference Function & Testing\n",
    "\n",
    "The SPROC prepared a model for us. We'll create now a UDF to apply inference on the `test_snowpark_df` (do you remember? We split the dataset at the beginning of the notebook)  \n",
    "\n",
    "#### Step 1: Define Python Function\n",
    "\n",
    "- Create an additional function `load_randomforest_model` to cache the model file.\n",
    "\n",
    "- Define `randomforest_predict`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012aab45-3de7-4812-bc4a-0554944cf962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the RandomForestClassifier model from file and cache the result\n",
    "@cachetools.cached(cache={})\n",
    "def load_randomforest_model(filename):\n",
    "    \n",
    "    # Import packages\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    # Get the import directory where the model file is stored\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    \n",
    "    # Get the import directory where the model file is stored\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "# Function to predict using the RandomForestClassifier model with handling missing values\n",
    "def randomforest_predict(X: pd.DataFrame) -> pd.Series:\n",
    "    \n",
    "    # Load the RandomForestClassifier model\n",
    "    model = load_randomforest_model(\"randomforest_player_classification_model.sav\")\n",
    "\n",
    "    # Handle missing values using SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = pd.Series(model.predict(X_imputed))\n",
    "\n",
    "    # Return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c25602-8b8e-49bd-9d1d-8076b1c48f3e",
   "metadata": {},
   "source": [
    "#### Step 2: Register UDF Function in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d2d70-82c5-4c14-88a8-62e24e06ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.udf.register(\n",
    "    func=randomforest_predict,\n",
    "    name=\"HOL_DB.HOL_SNOWPARK_FOOTBALL.udf_randomforest_predict\",\n",
    "    stage_location=\"@HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO\",\n",
    "    input_types=[T.FloatType()] * len(feature_cols),\n",
    "    return_type=T.FloatType(),\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    imports=[\"@HOL_DB.HOL_SNOWPARK_FOOTBALL.FOOTBALL_MODELS_REPO/randomforest_player_classification_model.sav\"],\n",
    "    packages=[\"scikit-learn\", \"joblib\", \"cachetools\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b3558-d234-4c4e-960d-82f6ca2543f3",
   "metadata": {},
   "source": [
    "#### Step 3: Inference Random Forest Classifier on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763eb2a5-0943-4777-9a8b-b7deb99b0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop records not having the feature populated.\n",
    "test_snowpark_df_drop_na = test_snowpark_df.dropna(subset=feature_cols)\n",
    "\n",
    "test_pred = test_snowpark_df_drop_na.select(\n",
    "    \"*\",\n",
    "    F.call_udf(\"HOL_DB.HOL_SNOWPARK_FOOTBALL.UDF_RANDOMFOREST_PREDICT\", [F.col(c) for c in feature_cols]).alias(\"prediction\")\n",
    ")\n",
    "test_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353ac6-ffbb-4e81-869a-ab05a7f1852e",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "In this section, we evaluate the performance of our trained model on the test data. We start by converting the Snowpark DataFrame `test_pred` into a Pandas DataFrame, then proceed to preprocess the data for evaluation. \n",
    "\n",
    "#### Step 1: Calculate Accuracy, Confusion Matrix, ROC-AUC\n",
    "This involves handling missing values, binarizing labels, calculating accuracy, generating a confusion matrix, and calculating the ROC-AUC score for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585dcc94-a91d-48fd-95e0-c6061af8b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_pred_reverted is a Snowpark DataFrame\n",
    "# Convert it to a Pandas DataFrame\n",
    "test_pred_pd = test_pred.toPandas()\n",
    "num_classes = 4\n",
    "# Drop rows with missing values in the target variable\n",
    "test_pred_pd = test_pred_pd.dropna(subset=[\"PREDICTION\"])\n",
    "test_pred_pd = test_pred_pd.dropna(subset=[\"POSITION_ENCODED\"])\n",
    "\n",
    "# Binarize the labels\n",
    "y_true = label_binarize(test_pred_pd[\"POSITION_ENCODED\"], classes=list(range(num_classes)))\n",
    "y_pred = label_binarize(test_pred_pd[\"PREDICTION\"], classes=list(range(num_classes)))\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy on test data: {accuracy:.2%}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "# Convert numeric labels back to original string labels\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, columns=range(num_classes), index=range(num_classes))\n",
    "\n",
    "# Calculate ROC-AUC for each class\n",
    "roc_auc_scores = []\n",
    "for i in range(y_true.shape[1]):\n",
    "    roc_auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "    roc_auc_scores.append(roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3da7db-268d-41f2-9ac4-ce8749cee343",
   "metadata": {},
   "source": [
    "#### Step 2: Plot and Analize Results\n",
    "\n",
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd03f8-a507-408d-89cd-699a29cd3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix and something else\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert numeric labels back to original string labels\n",
    "position_labels = ['Goalkeeper', 'Defender', 'Midfield', 'Attack']\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix_df, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = range(len(position_labels))\n",
    "plt.xticks(tick_marks, position_labels, ha='center')  # Center x-axis labels\n",
    "plt.yticks(tick_marks, position_labels, va='center')  # Center y-axis labels\n",
    "\n",
    "plt.xlabel('Predicted Position')\n",
    "plt.ylabel('True Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a5925-a3e4-463a-80c1-69d2d865c5dc",
   "metadata": {},
   "source": [
    "**ROC-AUC Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d408db-655c-446d-a0fd-53ff1189a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC-AUC Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(position_labels, roc_auc_scores, color='skyblue')\n",
    "plt.title('ROC-AUC Scores for Each Position')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.xticks(ha='center')  # Center x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902ed4-feee-4985-8965-e2fc74a99c5c",
   "metadata": {},
   "source": [
    "**Scatter Plot: Total Goals vs. Predicted Position**\n",
    "\n",
    "The scatter plot illustrates the relationship between the total number of goals and the predicted player positions. As observed in the Confusion Matrix, the majority of attacking players are correctly classified, underscoring the significance of the \"NUM_GOALS\" feature in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda9896-0160-466a-94be-ac2cfb83ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for Total Goals vs Predicted Position\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(test_pred_pd[\"TOTAL_GOALS\"], test_pred_pd[\"PREDICTION\"], c=test_pred_pd[\"POSITION_ENCODED\"], cmap='viridis')\n",
    "plt.title('Total Goals vs Predicted Position')\n",
    "plt.xlabel('Total Goals')\n",
    "plt.ylabel('Predicted Position')\n",
    "plt.yticks(list(position_labels.keys()), list(position_labels.values()))  # Set y-axis ticks as string labels\n",
    "plt.colorbar(scatter, ticks=list(position_labels.keys()), label='Position', format=position_labels.get)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b295b-ca6f-45bf-b6d7-d3147ae20fb2",
   "metadata": {},
   "source": [
    "# Model Performance Considerations\n",
    "\n",
    "As mentioned at the beginning, accuracy was not the objective today, yet it's insightful to consider a few observations:\n",
    "\n",
    "- In my test dataset, Jude Bellingham is predicted to be in the attacking position, while he is actually a midfielder. However, this prediction might be justified by his prolific performance in the last period.\n",
    "- In general, the current model tends to mix up the attack and midfield positions, and occasionally the goalkeeper and defender positions.\n",
    "- Notably, completely incorrect predictions, such as placing a goalkeeper in the attack, are minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06939704-26f7-45cd-ad09-6aa525a1e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, when\n",
    "\n",
    "# Select relevant columns and add a column for Correct/Wrong prediction\n",
    "result = (\n",
    "    test_pred\n",
    "    .select(\n",
    "        \"NAME\",\n",
    "        \"CURRENT_CLUB_NAME\",\n",
    "        \"POSITION\",\n",
    "        \"SUB_POSITION\",\n",
    "        when(col(\"PREDICTION\") == 0, \"Goalkeeper\")\n",
    "        .when(col(\"PREDICTION\") == 1, \"Defender\")\n",
    "        .when(col(\"PREDICTION\") == 2, \"Midfield\")\n",
    "        .when(col(\"PREDICTION\") == 3, \"Attack\")\n",
    "        .otherwise(\"Unknown\")\n",
    "        .alias(\"PREDICTED_POSITION\"),\n",
    "        when(col(\"PREDICTION\") == col(\"POSITION_ENCODED\"), \"Correct\")\n",
    "        .otherwise(\"Wrong\")\n",
    "        .alias(\"Prediction_Status\")\n",
    "    )\n",
    "    .orderBy(col(\"LATEST_MARKET_VALUE_IN_EUR\").desc())\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5a4fc-f891-4f84-90b8-f6b27406d2f6",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "I'm sure that a more extensive feature engineering might improve more and more the accuracy of the model. I'm looking forward to test your model! \n",
    "\n",
    "Have fun! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
